# 시스템 내 병목 탐색 및 장애 대응 시나리오

---

## 1. 시나리오 및 병목 탐색 과정    
이번 부하 테스트에서 도커 컨테이너의 리소스 설정과 테스트 전략에 따라 수행한 결과, 제한된 환경에서는 스파이크 트래픽에서 p95 응답 시간이 `3,830ms`로 급증하며 병목 현상이 발생했습니다. 
이로 인해 성능 지표가 `SLO 기준(p99 < 400ms)`을 초과하였으며, 특정 트래픽 조건에서는 Redis와 DB 커넥션 풀 부족으로 인한 병목 가능성이 발견되었습니다.

특히 `시나리오 2`에서는 스파이크 트래픽 동안 힙 메모리 부족으로 인해 `OOM(Out Of Memory)` 에러가 발생하며 시스템이 비정상 종료되었습니다. 
이를 통해 메모리 사용량 초과와 불완전한 캐싱 정책이 주요 원인임을 확인했습니다. 
추후에 GC 로그 분석과 힙 덤프를 통해 메모리 누수의 원인에 대해 파악해보고 수정해 볼 예정입니다.

<br/>

## 2. 주요 장애 시나리오

### OOM(Out Of Memory)
- **현상**  
  애플리케이션이 실행 중 힙 메모리가 부족하여 OOM 오류가 발생하며 비정상 종료


- **원인**
    - 메모리 캐시 미해제 또는 데이터 증가로 인한 누수
    - 대용량 데이터를 메모리에 적재하거나 컬렉션 관리 미흡
<br/>

### Slow Query 또는 I/O 병목
- **현상**  
  응답 시간이 p99 기준 400ms 이상 지속 상승하며 CPU와 메모리 사용률 급증
- **원인**
    - 최적화되지 않은 DB 쿼리 및 인덱스 부족
    - Redis 또는 DB 커넥션 풀 제한으로 인해 처리 속도 지연

<br/>

## 3. 장애 대응 프로세스

### 1) 장애 탐지
- **모니터링 시스템**
    - Prometheus + Grafana로 CPU, 메모리, Disk I/O, 네트워크 트래픽 모니터링
    - 주요 알림 조건
        - 메모리 사용률 85% 이상 지속
        - p99 응답 시간 400ms 초과 지속
        - Redis/DB 연결 실패 발생

- **로그 기반 탐지**
    - Spring Boot의 GlobalExceptionHandler를 통해 OOM 및 Unhandled Exception 발생 시 Slack 알림 전송

### 2) 장애 분류 및 전파
- **장애 등급 분류**
    - **Critical**: 서비스 전면 중단 또는 10분 이상 응답 지연
    - **Major**: 특정 기능에서 응답 지연
    - **Minor**: 고객 영향 없는 경미한 장애

- **전파 경로**
    - **Critical**: 전체 Slack 채널 알림과 담당자 긴급 연락
    - **Major**: Slack 알림과 Jira를 통한 티켓 등록
    - **Minor**: 로그 기록과 내부 보고

### 3) 장애 복구 및 보고
- **OOM 장애 복구 절차**
    1. 애플리케이션 재시작 및 Redis/DB 커넥션 초기화
    2. GC 로그 분석과 힙 덤프 분석으로 메모리 누수 원인 파악

- **Slow Query 병목 복구 절차**
    1. 문제 쿼리의 실행 계획 분석과 DB 인덱스 추가
    2. Redis 캐싱을 통한 DB 부하 완화

### 4) 장애 상황 해소 통지
- 장애 복구 완료 후 Slack 및 Jira를 통해 상황 종료 알림
- 장애 발생 시각, 원인, 복구 완료 시점 등을 고객에게 공지

### 5) 장애 회고
- 장애 회고 회의 진행 및 개선점 도출
- 메모리 최적화 도구 도입 및 Redis 모니터링 강화
- 장애 대응 프로세스 문서 업데이트

<br/><br/>
